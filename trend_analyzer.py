from collections import Counter, defaultdict
from typing import List, Dict, Tuple
import re
from datetime import datetime, timedelta


class TrendAnalyzer:
    def __init__(self):
        # Webã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.trend_keywords = {
            # AI/MLæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
            "ai_ml": [
                "llm", "large language model", "gpt", "transformer", "attention",
                "diffusion", "generative ai", "foundation model", "multimodal",
                "retrieval augmented generation", "rag", "fine-tuning", "rlhf",
                "chain of thought", "in-context learning", "few-shot", "zero-shot"
            ],
            # é–‹ç™ºæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
            "development": [
                "code generation", "copilot", "coding assistant", "automated testing",
                "ci/cd", "devops", "microservices", "serverless", "containerization",
                "kubernetes", "api", "rest", "graphql", "websocket", "real-time"
            ],
            # WebæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
            "web_tech": [
                "react", "vue", "angular", "svelte", "nextjs", "nuxt", "remix",
                "typescript", "javascript", "node.js", "deno", "bun",
                "tailwind", "css", "html", "pwa", "spa", "ssr", "ssg"
            ],
            # ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ãƒˆãƒ¬ãƒ³ãƒ‰
            "data_infra": [
                "database", "nosql", "sql", "postgresql", "mongodb", "redis",
                "elasticsearch", "vector database", "embedding", "search",
                "caching", "cdn", "cloud", "aws", "azure", "gcp", "edge computing"
            ],
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒˆãƒ¬ãƒ³ãƒ‰
            "security": [
                "security", "authentication", "authorization", "oauth", "jwt",
                "encryption", "vulnerability", "privacy", "gdpr", "compliance",
                "zero trust", "devsecops", "penetration testing", "threat detection"
            ],
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ»ç›£è¦–ãƒˆãƒ¬ãƒ³ãƒ‰
            "performance": [
                "performance", "optimization", "monitoring", "observability",
                "metrics", "logging", "tracing", "alerting", "sli", "slo",
                "load balancing", "scalability", "high availability", "latency"
            ]
        }
        
        # æ–°èˆˆæŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜é‡è¦åº¦ï¼‰
        self.emerging_keywords = [
            "webassembly", "wasm", "web3", "blockchain", "metaverse", "ar", "vr",
            "quantum computing", "edge ai", "federated learning", "mlops",
            "chatops", "gitops", "platform engineering", "developer experience"
        ]
    
    def extract_trends(self, papers: List[Dict], days_back: int = 7) -> Dict:
        """
        è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            papers: è«–æ–‡ãƒªã‚¹ãƒˆ
            days_back: æ¯”è¼ƒå¯¾è±¡ã¨ã™ã‚‹æ—¥æ•°
            
        Returns:
            ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æžçµæžœ
        """
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        category_counts = defaultdict(Counter)
        keyword_counts = Counter()
        emerging_counts = Counter()
        
        # è‘—è€…æ‰€å±žæ©Ÿé–¢ã‚«ã‚¦ãƒ³ãƒˆ
        institution_counts = Counter()
        
        # ç ”ç©¶åˆ†é‡Žã‚«ãƒ†ã‚´ãƒªã‚«ã‚¦ãƒ³ãƒˆ
        arxiv_category_counts = Counter()
        
        for paper in papers:
            text = (paper["title"] + " " + paper["summary"]).lower()
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for category, keywords in self.trend_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        category_counts[category][keyword] += 1
                        keyword_counts[keyword] += 1
            
            # æ–°èˆˆæŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for keyword in self.emerging_keywords:
                if keyword in text:
                    emerging_counts[keyword] += 1
            
            # è‘—è€…æ‰€å±žæ©Ÿé–¢ã‚’æŠ½å‡º
            for author in paper.get("authors", []):
                # ç°¡å˜ãªæ©Ÿé–¢åæŠ½å‡ºï¼ˆ@ä»¥é™ãŒã‚ã‚Œã°ï¼‰
                author_lower = author.lower()
                institutions = ["google", "meta", "microsoft", "amazon", "openai", "anthropic",
                              "stanford", "mit", "berkeley", "harvard", "carnegie mellon"]
                for inst in institutions:
                    if inst in author_lower:
                        institution_counts[inst] += 1
                        break
            
            # ArXivã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for category in paper.get("categories", []):
                arxiv_category_counts[category] += 1
        
        return {
            "category_trends": dict(category_counts),
            "top_keywords": keyword_counts.most_common(10),
            "emerging_tech": emerging_counts.most_common(5),
            "active_institutions": institution_counts.most_common(5),
            "research_areas": arxiv_category_counts.most_common(8),
            "total_papers": len(papers)
        }
    
    def generate_trend_summary(self, trends: Dict) -> str:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æžçµæžœã‹ã‚‰è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        """
        summary_parts = []
        
        # å…¨ä½“çµ±è¨ˆ
        summary_parts.append(f"ðŸ“Š **ä»Šæ—¥ã®ç ”ç©¶å‹•å‘** (å¯¾è±¡: {trends['total_papers']}ä»¶ã®è«–æ–‡)")
        summary_parts.append("")
        
        # ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if trends["top_keywords"]:
            summary_parts.append("ðŸ”¥ **æ³¨ç›®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP5:**")
            for i, (keyword, count) in enumerate(trends["top_keywords"][:5], 1):
                summary_parts.append(f"  {i}. **{keyword.title()}** ({count}ä»¶)")
        
        summary_parts.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰
        category_names = {
            "ai_ml": "ðŸ¤– AI/MLæŠ€è¡“",
            "development": "âš™ï¸ é–‹ç™ºæŠ€è¡“", 
            "web_tech": "ðŸŒ WebæŠ€è¡“",
            "data_infra": "ðŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©",
            "security": "ðŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
            "performance": "âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹"
        }
        
        summary_parts.append("ðŸ“ˆ **åˆ†é‡Žåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰:**")
        for category, name in category_names.items():
            if category in trends["category_trends"] and trends["category_trends"][category]:
                top_keyword = trends["category_trends"][category].most_common(1)[0]
                total_count = sum(trends["category_trends"][category].values())
                summary_parts.append(f"  {name}: **{top_keyword[0].title()}** ({total_count}ä»¶)")
        
        summary_parts.append("")
        
        # æ–°èˆˆæŠ€è¡“
        if trends["emerging_tech"]:
            summary_parts.append("ðŸš€ **æ–°èˆˆæŠ€è¡“å‹•å‘:**")
            for keyword, count in trends["emerging_tech"]:
                summary_parts.append(f"  â€¢ **{keyword.title()}** ({count}ä»¶)")
            summary_parts.append("")
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªç ”ç©¶æ©Ÿé–¢
        if trends["active_institutions"]:
            summary_parts.append("ðŸ›ï¸ **æ´»ç™ºãªç ”ç©¶æ©Ÿé–¢:**")
            for inst, count in trends["active_institutions"]:
                summary_parts.append(f"  â€¢ **{inst.title()}** ({count}ä»¶)")
            summary_parts.append("")
        
        # ç ”ç©¶åˆ†é‡Ž
        if trends["research_areas"]:
            summary_parts.append("ðŸ“š **æ´»ç™ºãªç ”ç©¶åˆ†é‡Ž:**")
            area_names = {
                "cs.AI": "äººå·¥çŸ¥èƒ½", "cs.LG": "æ©Ÿæ¢°å­¦ç¿’", "cs.CL": "è‡ªç„¶è¨€èªžå‡¦ç†",
                "cs.SE": "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦", "cs.HC": "HCI", "cs.IR": "æƒ…å ±æ¤œç´¢",
                "cs.DB": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "cs.DC": "åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ", "cs.PL": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªž",
                "cs.CR": "æš—å·ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "cs.CV": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³"
            }
            for area, count in trends["research_areas"][:5]:
                display_name = area_names.get(area, area)
                summary_parts.append(f"  â€¢ **{display_name}** ({count}ä»¶)")
        
        return "\n".join(summary_parts)
    
    def detect_trend_changes(self, current_papers: List[Dict], 
                           historical_papers: List[Dict] = None) -> str:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
        """
        if not historical_papers:
            return "ðŸ“… **ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–**: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚æ¯”è¼ƒåˆ†æžã¯æ¬¡å›žã‹ã‚‰åˆ©ç”¨å¯èƒ½ã§ã™"
        
        current_trends = self.extract_trends(current_papers)
        historical_trends = self.extract_trends(historical_papers)
        
        # ç°¡å˜ãªå¢—æ¸›åˆ†æž
        changes = []
        for keyword, count in current_trends["top_keywords"][:5]:
            # éŽåŽ»ãƒ‡ãƒ¼ã‚¿ã§ã®å‡ºç¾å›žæ•°ã‚’æ¤œç´¢
            historical_count = 0
            for hist_keyword, hist_count in historical_trends["top_keywords"]:
                if hist_keyword == keyword:
                    historical_count = hist_count
                    break
            
            if historical_count == 0:
                changes.append(f"  ðŸ†• **{keyword.title()}** (æ–°ç™»å ´)")
            elif count > historical_count:
                changes.append(f"  ðŸ“ˆ **{keyword.title()}** (+{count - historical_count}ä»¶)")
            elif count < historical_count:
                changes.append(f"  ðŸ“‰ **{keyword.title()}** (-{historical_count - count}ä»¶)")
        
        if changes:
            return "ðŸ“… **ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–:**\n" + "\n".join(changes)
        else:
            return "ðŸ“… **ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–**: å¤§ããªå¤‰å‹•ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“"